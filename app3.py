"""
Streamlit AI-Agent for CSV analysis
Single-file app implementing:
- CSV upload + validation
- Data cleaning & preprocessing
- Automated EDA (summary, missingness, correlations, distributions)
- User goal input (clarification if ambiguous)
- Analysis planning + insight generation (rule-based)
- Visualization creation + recommendation
- "Run Full Analysis" single-click mode
- Optional OpenAI summarization if API key provided

Author: Generated by ChatGPT
"""

import io
import os
import math
import base64
import tempfile
import textwrap
from typing import List, Tuple, Dict, Optional

import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from datetime import datetime

# Optional: OpenAI summarization (user can input key)
try:
    import openai
    OPENAI_AVAILABLE = True
except Exception:
    OPENAI_AVAILABLE = False

st.set_page_config(page_title="AI Agent â€” Automated CSV Analysis", layout="wide", initial_sidebar_state="expanded")

##############################################################################
# Utilities
##############################################################################

MAX_UPLOAD_MB = 30  # file size limit
ALLOWED_EXTENSIONS = {'.csv', '.txt'}

def filesize_mb(uploaded_file) -> float:
    uploaded_file.seek(0, io.SEEK_END)
    size = uploaded_file.tell() / (1024 * 1024)
    uploaded_file.seek(0)
    return size

def file_extension_ok(filename: str) -> bool:
    _, ext = os.path.splitext(filename.lower())
    return ext in ALLOWED_EXTENSIONS

def read_csv_safe(uploaded_file) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """Try reading a CSV robustly, support different encodings & separators."""
    try:
        uploaded_file.seek(0)
        # try pandas default
        df = pd.read_csv(uploaded_file)
        return df, None
    except Exception as e1:
        # attempt with common separators
        uploaded_file.seek(0)
        content = uploaded_file.read().decode(errors='ignore')
        for sep in [',', ';', '\t', '|']:
            try:
                df = pd.read_csv(io.StringIO(content), sep=sep)
                return df, None
            except Exception:
                continue
        return None, f"Could not parse CSV. Error: {e1}"

def detect_datetime_columns(df: pd.DataFrame, max_to_check=10):
    datetimes = []
    for col in df.columns:
        if df[col].dtype == 'object':
            sample = df[col].dropna().astype(str).head(max_to_check)
            parsed = 0
            for s in sample:
                try:
                    pd.to_datetime(s)
                    parsed += 1
                except Exception:
                    pass
            if len(sample) > 0 and parsed / len(sample) > 0.6:
                datetimes.append(col)
    return datetimes

def human_readable_bytes(num_bytes: int):
    for unit in ['B','KB','MB','GB','TB']:
        if num_bytes < 1024.0:
            return f"{num_bytes:.2f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.2f} PB"

def download_link(object_to_download, download_filename, download_link_text):
    """
    Generates a link to download the given object_to_download.
    object_to_download: bytes OR str
    """
    if isinstance(object_to_download, bytes):
        b64 = base64.b64encode(object_to_download).decode()
    else:
        b64 = base64.b64encode(object_to_download.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{download_filename}">{download_link_text}</a>'

##############################################################################
# Preprocessing & EDA functions
##############################################################################

def basic_cleaning(df: pd.DataFrame, drop_threshold=0.9) -> Tuple[pd.DataFrame, Dict]:
    """
    - Drops columns with > drop_threshold missing fraction
    - Converts detected datetime-like columns to datetime
    - Returns cleaned df and a log dict
    """
    log = {}
    n_rows, n_cols = df.shape
    # Drop extremely sparse columns
    missing_frac = df.isna().mean()
    to_drop = missing_frac[missing_frac > drop_threshold].index.tolist()
    df = df.drop(columns=to_drop)
    log['dropped_columns_due_to_missingness'] = to_drop

    # Convert object columns that look like numbers with commas
    for col in df.select_dtypes(include='object').columns:
        sample = df[col].dropna().astype(str).head(50)
        if sample.str.match(r'^[0-9,\.]+$').mean() > 0.6:
            # try to convert removing commas
            try:
                df[col] = sample.map(lambda x: x.replace(',', '')).astype(float)
            except Exception:
                pass

    # Datetime detection
    dt_cols = detect_datetime_columns(df)
    for col in dt_cols:
        try:
            df[col] = pd.to_datetime(df[col], errors='coerce')
        except Exception:
            pass
    log['detected_datetime_columns'] = dt_cols

    # Basic missing value imputation hints (not applied here)
    log['missing_summary'] = df.isna().sum().to_dict()
    return df, log

def generate_summary(df: pd.DataFrame) -> Dict:
    summary = {}
    summary['shape'] = df.shape
    summary['dtypes'] = df.dtypes.apply(lambda x: str(x)).to_dict()
    summary['missing'] = df.isna().sum().to_dict()
    numeric = df.select_dtypes(include=[np.number])
    if not numeric.empty:
        summary['numeric_describe'] = numeric.describe().T.to_dict()
        # Correlations
        try:
            corr = numeric.corr()
            summary['correlation_top_abs'] = (
                corr.abs()
                    .unstack()
                    .sort_values(ascending=False)
                    .reset_index()
                    .query('level_0 != level_1')
                    .drop_duplicates(subset=['level_0','level_1'])
                    .head(10)
                    .to_dict(orient='records')
            )
        except Exception:
            summary['correlation_top_abs'] = []
    else:
        summary['numeric_describe'] = {}
        summary['correlation_top_abs'] = []
    return summary

def detect_ambiguous_goal(goal_text: str) -> bool:
    """Heuristic: too short, too vague or no keywords -> ambiguous"""
    if not goal_text or len(goal_text.strip()) < 8:
        return True
    keywords = ['predict', 'trend', 'compare', 'correl', 'relationship', 'segment', 'group', 'forecast', 'outliers', 'anomaly', 'visualize']
    text_l = goal_text.lower()
    if not any(k in text_l for k in keywords):
        return True
    return False

def recommend_chart_types(df: pd.DataFrame, goal_text: str) -> List[Tuple[str, str]]:
    """
    Return list of recommended (chart_name, reason)
    """
    recs = []
    numeric = df.select_dtypes(include=np.number).columns.tolist()
    cate = df.select_dtypes(include=['object', 'category']).columns.tolist()
    dt = df.select_dtypes(include=['datetime']).columns.tolist()
    goal = (goal_text or "").lower()

    if any(k in goal for k in ['trend', 'time', 'forecast', 'over time']) and dt:
        recs.append(("Line chart", f"Time-series trend on {dt[0]}"))
    if 'compare' in goal and len(cate)>0 and len(numeric)>0:
        recs.append(("Grouped bar chart", f"Compare {numeric[0]} across {cate[0]}"))
    if 'correl' in goal and len(numeric)>=2:
        recs.append(("Scatter plot", f"Correlation between {numeric[0]} and {numeric[1]}"))
    if 'segment' in goal or 'cluster' in goal:
        recs.append(("Scatter with clusters", "Pairwise scatter/cluster analysis for segmentation"))
    if 'outlier' in goal or 'anomaly' in goal:
        recs.append(("Boxplot & anomaly detection", "Boxplots plus IsolationForest to highlight outliers"))
    # general EDA suggestions
    if not recs:
        if numeric:
            recs.append(("Histogram / KDE", f"Distribution of {numeric[0]}"))
        if cate:
            recs.append(("Bar chart", f"Counts of {cate[0]}"))
    return recs

def run_anomaly_detection(df: pd.DataFrame, n_estimators=100):
    """
    Run IsolationForest on numeric subset to flag possible anomalies.
    Returns flag_series (index aligned) and model summary
    """
    numeric = df.select_dtypes(include=np.number)
    if numeric.shape[1] < 1 or numeric.shape[0] < 5:
        return pd.Series([False]*len(df), index=df.index), {'note': 'Not enough numeric data'}
    # drop rows with NA for the purpose of anomaly detection temporarily
    numeric_filled = numeric.fillna(numeric.median())
    iso = IsolationForest(n_estimators=n_estimators, random_state=42)
    flags = iso.fit_predict(numeric_filled)
    anomaly_flag = pd.Series(flags == -1, index=df.index)
    score = iso.decision_function(numeric_filled)
    return anomaly_flag, {'model': 'IsolationForest', 'anomaly_fraction': anomaly_flag.mean()}

def regression_quick_try(df: pd.DataFrame, target_col: str):
    """
    Quick linear regression attempt if target is numeric and we have numeric features.
    Returns simple metrics and coefficients.
    """
    if target_col not in df.columns:
        return {'error': 'target not in df'}
    y = df[target_col]
    X = df.select_dtypes(include=np.number).drop(columns=[target_col], errors='ignore')
    if X.shape[1] < 1 or y.dtype.kind not in 'fi' or X.shape[0] < 10:
        return {'error': 'Not enough numeric data or samples for regression'}
    # simple impute
    X_f = X.fillna(X.median())
    y_f = y.fillna(y.median())
    X_train, X_test, y_train, y_test = train_test_split(X_f, y_f, test_size=0.2, random_state=42)
    model = LinearRegression()
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)
    coefs = dict(zip(X.columns, model.coef_.tolist()))
    return {'r2': r2, 'coefs': coefs, 'intercept': float(model.intercept_)}

def openai_summarize(text: str, api_key: str, prompt: str="Summarize the following EDA findings in clear language:"):
    """
    Optional helper to summarize using OpenAI. If openai package present and api_key provided.
    """
    if not OPENAI_AVAILABLE:
        return "OpenAI package not installed or available."
    openai.api_key = api_key
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini", # user can change
            messages=[
                {"role":"system", "content": "You are a helpful summarizer."},
                {"role":"user", "content": prompt + "\n\n" + text}
            ],
            max_tokens=400,
            temperature=0.2
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"OpenAI summarization failed: {e}"

##############################################################################
# UI helpers
##############################################################################

def show_dataframe(df: pd.DataFrame):
    st.dataframe(df, use_container_width=True)

def plot_missingness_heatmap(df: pd.DataFrame, width=700, height=350):
    miss = df.isna().astype(int)
    fig = px.imshow(miss.T, aspect='auto', labels=dict(x="Row index", y="Columns", color="Missing"))
    fig.update_layout(height=height, width=width)
    st.plotly_chart(fig, use_container_width=True)

def plot_correlation_matrix(df: pd.DataFrame):
    numeric = df.select_dtypes(include=np.number)
    if numeric.shape[1] < 2:
        st.info("Not enough numeric columns to compute correlation matrix.")
        return
    corr = numeric.corr()
    fig = px.imshow(corr, text_auto=True)
    st.plotly_chart(fig, use_container_width=True)

def display_recommended_charts(df: pd.DataFrame, recs: List[Tuple[str, str]]):
    for chart_name, reason in recs:
        st.markdown(f"**{chart_name}** â€” {reason}")
        # show example for each recommended type if possible
        try:
            if chart_name == "Line chart":
                dt_cols = df.select_dtypes(include=['datetime']).columns
                num_cols = df.select_dtypes(include=np.number).columns
                if len(dt_cols) and len(num_cols):
                    fig = px.line(df, x=dt_cols[0], y=num_cols[0], title=f"{num_cols[0]} over {dt_cols[0]}")
                    st.plotly_chart(fig, use_container_width=True)
            elif chart_name == "Grouped bar chart":
                cate = df.select_dtypes(include=['object','category']).columns
                num = df.select_dtypes(include=np.number).columns
                if len(cate) and len(num):
                    agg = df.groupby(cate[0])[num[0]].mean().reset_index().sort_values(num[0], ascending=False).head(30)
                    fig = px.bar(agg, x=cate[0], y=num[0], title=f"Average {num[0]} by {cate[0]}")
                    st.plotly_chart(fig, use_container_width=True)
            elif chart_name == "Scatter plot":
                num = df.select_dtypes(include=np.number).columns
                if len(num) >= 2:
                    fig = px.scatter(df, x=num[0], y=num[1], trendline="ols")
                    st.plotly_chart(fig, use_container_width=True)
            elif chart_name == "Histogram / KDE":
                num = df.select_dtypes(include=np.number).columns
                if len(num):
                    fig = px.histogram(df, x=num[0], nbins=40, marginal="box")
                    st.plotly_chart(fig, use_container_width=True)
            elif chart_name == "Bar chart":
                cate = df.select_dtypes(include=['object','category']).columns
                if len(cate):
                    counts = df[cate[0]].value_counts().reset_index().rename(columns={'index':cate[0], cate[0]:'count'})
                    fig = px.bar(counts.head(30), x=cate[0], y='count')
                    st.plotly_chart(fig, use_container_width=True)
            elif chart_name == "Boxplot & anomaly detection":
                num = df.select_dtypes(include=np.number).columns
                if len(num):
                    fig = px.box(df, y=num[0], points="outliers")
                    st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.warning(f"Could not create example for {chart_name}: {e}")

##############################################################################
# Main App
##############################################################################

def main():
    st.title("ðŸš€ AI Agent â€” Automated CSV Analysis")
    st.markdown("Upload a CSV and let the AI Agent clean, analyze, visualize and produce a report. "
                "Use the **Run Full Analysis** button to do everything with a single click.")

    # Sidebar controls
    st.sidebar.header("Settings & Controls")
    st.sidebar.markdown("**Upload & preprocessing settings**")
    drop_thresh = st.sidebar.slider("Drop columns with > missing fraction", 0.5, 1.0, 0.9, 0.01)
    max_rows_preview = st.sidebar.number_input("Max rows to preview", min_value=5, max_value=1000, value=100)
    st.sidebar.markdown("---")
    st.sidebar.markdown("**Automation**")
    auto_mode = st.sidebar.checkbox("Single-click: Run Full Analysis (auto)", value=False)
    st.sidebar.markdown("---")
    st.sidebar.markdown("**Optional: OpenAI** (for nicer plain-language summaries)")
    openai_key = st.sidebar.text_input("OpenAI API Key (optional)", type="password")
    use_openai = st.sidebar.checkbox("Use OpenAI for summary (optional)", value=False)
    if use_openai and not openai_key:
        st.sidebar.warning("Provide an API key if you wish to use OpenAI summarization.")
    st.sidebar.markdown("---")
    st.sidebar.markdown("Tip: Hover tooltips and click charts to inspect interactively.")

    # File upload
    uploaded_file = st.file_uploader("Upload CSV file", type=['csv','txt'])

    df = None
    file_ok = False
    if uploaded_file is not None:
        fname = uploaded_file.name
        if not file_extension_ok(fname):
            st.error("Invalid file extension. Please upload a .csv or .txt file.")
        else:
            size_mb = filesize_mb(uploaded_file)
            if size_mb > MAX_UPLOAD_MB:
                st.error(f"Uploaded file is too large ({size_mb:.1f} MB). Limit: {MAX_UPLOAD_MB} MB.")
                st.info("Consider uploading a smaller sample or compressing your dataset.")
                uploaded_file = None
            else:
                df, err = read_csv_safe(uploaded_file)
                if df is None:
                    st.error(err or "Failed to read CSV.")
                else:
                    file_ok = True
                    st.success(f"Loaded `{fname}` â€” {df.shape[0]} rows Ã— {df.shape[1]} columns â€” size {size_mb:.2f} MB")
                    st.caption("Preview (first rows):")
                    show_dataframe(df.head(int(max_rows_preview)))

    # If in auto mode and file present, run pipeline on button click OR auto immediately
    run_full = False
    if auto_mode:
        # show button but default to run automatically if a file is present (makes 'single click' semantics)
        if file_ok:
            if st.button("Run Full Analysis Now"):
                run_full = True
        else:
            st.info("Upload a CSV before running full analysis.")
    else:
        # normal mode: show step-by-step controls
        pass

    # Allow manual step running
    st.header("Analysis Pipeline")
    col1, col2 = st.columns([1,1])

    if file_ok:
        with col1:
            if st.button("1) Clean & Preprocess"):
                st.session_state['_do_clean'] = True
            else:
                st.session_state.setdefault('_do_clean', False)
        with col2:
            if st.button("2) Run EDA"):
                st.session_state['_do_eda'] = True
            else:
                st.session_state.setdefault('_do_eda', False)

        # If auto or step buttons pressed
        if run_full:
            st.session_state['_do_clean'] = True
            st.session_state['_do_eda'] = True
            st.session_state['_do_goal'] = True
            st.session_state['_do_analysis'] = True

        if st.session_state.get('_do_clean', False):
            st.subheader("Cleaning & Preprocessing")
            with st.spinner("Cleaning data..."):
                cleaned_df, clean_log = basic_cleaning(df.copy(), drop_threshold=drop_thresh)
            st.success("Cleaning completed.")
            st.write("Changes summary:")
            st.json(clean_log)
            st.write("Preview cleaned data:")
            show_dataframe(cleaned_df.head(int(max_rows_preview)))
        else:
            cleaned_df = None

        if st.session_state.get('_do_eda', False):
            if cleaned_df is None:
                cleaned_df = df.copy()
            st.subheader("Exploratory Data Analysis (EDA)")
            with st.spinner("Running EDA..."):
                summary = generate_summary(cleaned_df)
            st.markdown("**Dataset summary**")
            st.write(f"Rows Ã— Columns: {summary['shape']}")
            st.write("Column types:")
            st.json(summary['dtypes'])
            st.write("Missing values (top columns):")
            missing_df = pd.Series(summary['missing']).sort_values(ascending=False).head(10)
            st.table(missing_df)

            # show numeric describe
            if summary['numeric_describe']:
                nd = pd.DataFrame(summary['numeric_describe']).T
                st.markdown("**Numeric summary (first 10):**")
                st.dataframe(nd.head(10), use_container_width=True)
            # show correlation
            st.markdown("**Correlation matrix**")
            plot_correlation_matrix(cleaned_df)
            st.markdown("**Missingness heatmap**")
            plot_missingness_heatmap(cleaned_df)

            # top correlations
            if summary['correlation_top_abs']:
                st.markdown("Top absolute correlations (sample):")
                st.json(summary['correlation_top_abs'])

            # recommend chart types
            st.markdown("**Recommended chart types (based on data & goal)**")
            # placeholder goal
            user_goal_preview = st.text_input("Briefly, what do you want from this data? (e.g., 'Find trends in sales over time')", value="")
            recs = recommend_chart_types(cleaned_df, user_goal_preview)
            display_recommended_charts(cleaned_df, recs)

        # Goal input & ambiguity check
        st.subheader("User Goal / Question")
        user_goal = st.text_input("Enter your goal or question about the data (e.g., 'Show me sales trends over time, forecast next quarter')", value="")
        clarify_text = ""
        ambiguous = detect_ambiguous_goal(user_goal)
        if ambiguous:
            st.warning("The goal looks ambiguous or too short. The agent will ask for clarification.")
            clarify_text = st.text_area("Please clarify your goal (give more details, what you'd like to predict/compare/visualize):", value="")
            if clarify_text.strip():
                user_goal = clarify_text

        # Optionally run analysis & insight generation
        if st.button("Run Analysis / Generate Insights") or st.session_state.get('_do_analysis', False):
            if cleaned_df is None:
                cleaned_df = df.copy()
            st.subheader("Analysis Planning & Execution")
            with st.spinner("Planning analysis..."):
                # Plan based on heuristics
                plan_notes = []
                if 'predict' in user_goal.lower() or 'forecast' in user_goal.lower():
                    plan_notes.append("Attempt a simple forecasting or regression if time series / numeric target found.")
                if 'segment' in user_goal.lower() or 'cluster' in user_goal.lower():
                    plan_notes.append("Perform segmentation heuristics using numeric pairwise comparisons.")
                if not plan_notes:
                    plan_notes.append("Perform correlation checks, univariate distributions, outlier detection, and recommend visuals.")

            st.markdown("**Planned steps:**")
            for p in plan_notes:
                st.write("- " + p)

            with st.spinner("Generating insights..."):
                anomaly_flags, anomaly_info = run_anomaly_detection(cleaned_df)
                sample_anom = anomaly_flags.sum()
                insight_texts = []
                if sample_anom > 0:
                    insight_texts.append(f"Detected {int(sample_anom)} potential anomalous rows (IsolationForest).")
                else:
                    insight_texts.append("No major anomalies detected by IsolationForest.")

                # quick regress attempt if goal mentions predict and numeric present
                quick_reg_result = {}
                numeric_cols = cleaned_df.select_dtypes(include=np.number).columns.tolist()
                if 'predict' in user_goal.lower() and len(numeric_cols) >= 2:
                    # pick a plausible target â€” numeric with most non-null values
                    target = cleaned_df[numeric_cols].notna().sum().sort_values(ascending=False).index[0]
                    quick_reg_result = regression_quick_try(cleaned_df, target)
                    if 'r2' in quick_reg_result:
                        insight_texts.append(f"Quick regression on `{target}` produced R^2={quick_reg_result['r2']:.3f}.")
                    else:
                        insight_texts.append("Regression attempt not possible or inconclusive.")

                # choose recommended charts again
                final_recs = recommend_chart_types(cleaned_df, user_goal)
                st.markdown("**Agent Insights (raw):**")
                for t in insight_texts:
                    st.write("- " + t)

            # Optional OpenAI summarization
            if use_openai and openai_key:
                combined_text = "\n".join(insight_texts + [str(final_recs)])
                with st.spinner("Summarizing insights with OpenAI..."):
                    summary_text = openai_summarize(combined_text, openai_key)
                st.markdown("**Natural-language summary (OpenAI)**")
                st.write(summary_text)
            else:
                st.markdown("**Natural-language summary (agent-generated)**")
                st.write(" ".join(insight_texts))

            st.subheader("Recommended Visualizations")
            display_recommended_charts(cleaned_df, final_recs)

            # Show anomalies table if any
            if anomaly_flags.any():
                st.subheader("Anomalies (sample rows flagged)")
                show_dataframe(cleaned_df[anomaly_flags].head(200))

            # Final report & export
            st.subheader("Report & Export")
            report_lines = [
                f"Report generated: {datetime.utcnow().isoformat()}Z",
                f"File: {uploaded_file.name}",
                f"Rows Ã— Columns: {cleaned_df.shape}",
                "Key insights:",
            ] + insight_texts
            report_text = "\n".join(report_lines)
            st.markdown("**Plain report preview:**")
            st.text(report_text)

            # Download link
            st.markdown(download_link(report_text, "ai_agent_report.txt", "Download report (.txt)"), unsafe_allow_html=True)

    else:
        st.info("Upload a CSV file to begin. The agent will validate the file format and size and then guide you through cleaning and analysis.")

    # Footer / help
    st.sidebar.markdown("---")
    st.sidebar.markdown("Need a custom change? For example:")
    st.sidebar.markdown("- Add a specific ML model (classification/regression) with hyperparameter tuning\n- Produce a PDF report instead of .txt\n- Connect to a database or cloud storage\n\nIf you'd like, I can add that into the app for you.")

if __name__ == "__main__":
    main()
